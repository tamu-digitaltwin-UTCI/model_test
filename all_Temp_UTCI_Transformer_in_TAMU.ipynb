{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "此部分用于梳理实验的逻辑，我们的实验更新了关于时序数据的部分，增大了实验的数据feature"
      ],
      "metadata": {
        "id": "0Qyg8fmNDxNw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "第一步是分割图像并计算每个region的tree_height,building_height"
      ],
      "metadata": {
        "id": "UqREszSoEwEs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKLJqD5IDCWT",
        "outputId": "05d1eb82-ec33-4dfc-e930-d3f6ffc87ebb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rasterio in /usr/local/lib/python3.10/dist-packages (1.3.10)\n",
            "Requirement already satisfied: affine in /usr/local/lib/python3.10/dist-packages (from rasterio) (2.4.0)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.10/dist-packages (from rasterio) (23.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from rasterio) (2024.7.4)\n",
            "Requirement already satisfied: click>=4.0 in /usr/local/lib/python3.10/dist-packages (from rasterio) (8.1.7)\n",
            "Requirement already satisfied: cligj>=0.5 in /usr/local/lib/python3.10/dist-packages (from rasterio) (0.7.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rasterio) (1.25.2)\n",
            "Requirement already satisfied: snuggs>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from rasterio) (1.4.7)\n",
            "Requirement already satisfied: click-plugins in /usr/local/lib/python3.10/dist-packages (from rasterio) (1.1.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from rasterio) (71.0.4)\n",
            "Requirement already satisfied: pyparsing>=2.1.6 in /usr/local/lib/python3.10/dist-packages (from snuggs>=1.4.1->rasterio) (3.1.2)\n"
          ]
        }
      ],
      "source": [
        "pip install rasterio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Afxtqri3FNf1",
        "outputId": "6cd940f8-04de-4709-b690-41a21b54b619"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# 检查文件是否存在\n",
        "file_path = '/content/drive/MyDrive/UTCI_prediction/data/spatial_images/landcover9.tif'\n",
        "if os.path.exists(file_path):\n",
        "    print(\"文件存在\")\n",
        "else:\n",
        "    print(\"文件不存在，检查路径或文件名\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NtP0TTwPJxQQ",
        "outputId": "b3d35949-acee-4dc5-8a62-b366bd1f78c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "文件存在\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import rasterio\n",
        "from skimage.measure import label, regionprops\n",
        "from skimage.filters import threshold_otsu\n",
        "\n",
        "def read_tif(file_path):\n",
        "    \"\"\"读取TIF文件并返回数据数组。\"\"\"\n",
        "    with rasterio.open(file_path) as src:\n",
        "        return src.read(1)  # 读取第一层数据\n",
        "\n",
        "def segment_by_landcover(image):\n",
        "    \"\"\"Segment the landcover image based on its unique categorical values.\"\"\"\n",
        "    labels = label(image)  # Automatically label each area\n",
        "    return labels\n",
        "\n",
        "# 读取空间数据\n",
        "landcover = read_tif('/content/drive/MyDrive/UTCI_prediction/data/spatial_images/landcover9.tif')\n",
        "tree_height = read_tif('/content/drive/MyDrive/UTCI_prediction/data/spatial_images/tree height_9.tif')\n",
        "building_height = read_tif('/content/drive/MyDrive/UTCI_prediction/data/spatial_images/building height_9.tif')\n",
        "dem = read_tif('/content/drive/MyDrive/UTCI_prediction/data/spatial_images/DEM_9.tif')\n",
        "\n",
        "segmented_landcover = segment_by_landcover(landcover)\n",
        "\n",
        "# 读取CSV文件\n",
        "csv_data = pd.read_csv('/content/drive/MyDrive/UTCI_prediction/data/Yifan_updated_data_with_timestamps.csv')\n",
        "\n",
        "features_list = []\n",
        "regions = regionprops(segmented_landcover, intensity_image=tree_height)\n",
        "\n",
        "for index, row in csv_data.iterrows():\n",
        "    for region in regions:\n",
        "        features = {\n",
        "            'region_id': region.label,\n",
        "            'tree_height_mean': np.mean(tree_height[region.coords[:, 0], region.coords[:, 1]]),\n",
        "            'building_height_mean': np.mean(building_height[region.coords[:, 0], region.coords[:, 1]]),\n",
        "            'dem_mean': np.mean(dem[region.coords[:, 0], region.coords[:, 1]]),\n",
        "            'timestamp': row['timestamp']\n",
        "        }\n",
        "\n",
        "\n",
        "        for col in csv_data.columns[1:]:\n",
        "            features[col] = row[col]\n",
        "\n",
        "        features_list.append(features)\n",
        "\n",
        "all_features_df = pd.DataFrame(features_list)\n",
        "\n",
        "# 将所有特征数据保存到一个CSV文件中\n",
        "all_features_df.to_csv('/content/drive/MyDrive/UTCI_prediction/data/feature_output.csv', index=False)"
      ],
      "metadata": {
        "id": "jOxc6M--IuQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "提取UTCI数据，前期都是数据预处理部分，期待把两个数据集合一"
      ],
      "metadata": {
        "id": "yXurBV8hTJEE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import rasterio\n",
        "from skimage.measure import label, regionprops\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "def read_utci_from_tif(file_path):\n",
        "    \"\"\"专门用于从TIF文件读取UTCi数据的函数。\"\"\"\n",
        "    with rasterio.open(file_path) as src:\n",
        "        utci_data = src.read(1)\n",
        "    return utci_data\n",
        "\n",
        "def segment_by_landcover(image):\n",
        "    \"\"\"根据土地利用的唯一分类值进行分割。\"\"\"\n",
        "    labels = label(image)\n",
        "    return labels\n",
        "\n",
        "# 读取土地利用数据并进行分割\n",
        "landcover_path = '/content/drive/MyDrive/UTCI_prediction/data/spatial_images/landcover9.tif'\n",
        "landcover = read_utci_from_tif(landcover_path)\n",
        "segmented_landcover = segment_by_landcover(landcover)\n",
        "\n",
        "# 新数据集文件夹路径\n",
        "new_dataset_folder = '/content/drive/MyDrive/UTCI_prediction/data/output_images'\n",
        "features_list = []\n",
        "\n",
        "\n",
        "initial_timestamp = datetime.strptime(\"2022-07-03-00\", \"%Y-%m-%d-%H\")\n",
        "time_increment = timedelta(hours=1)\n",
        "\n",
        "# 遍历新数据集中的每个文件\n",
        "for index, filename in enumerate(sorted(os.listdir(new_dataset_folder))):\n",
        "    if filename.endswith('.tif'):\n",
        "        new_data_path = os.path.join(new_dataset_folder, filename)\n",
        "        new_data = read_utci_from_tif(new_data_path)\n",
        "\n",
        "\n",
        "        current_timestamp = initial_timestamp + index * time_increment\n",
        "        timestamp_str = current_timestamp.strftime(\"%Y-%m-%d-%H\")\n",
        "\n",
        "        regions = regionprops(segmented_landcover, intensity_image=new_data)\n",
        "        for region in regions:\n",
        "            utc_values = new_data[region.coords[:, 0], region.coords[:, 1]]\n",
        "            utc_mean = np.mean(utc_values) if utc_values.size > 0 else np.nan\n",
        "\n",
        "            features = {\n",
        "                'region_id': region.label,\n",
        "                'UTCI': utc_mean,\n",
        "                'timestamp': timestamp_str\n",
        "            }\n",
        "\n",
        "            features_list.append(features)\n",
        "\n",
        "all_features_df = pd.DataFrame(features_list)\n",
        "output_csv_path = '/content/drive/MyDrive/UTCI_prediction/data/UTCI_feature.csv'\n",
        "all_features_df.to_csv(output_csv_path, index=False)"
      ],
      "metadata": {
        "id": "yuuSIyMsTQfD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "这部分开始训练模型，前面的数据准备工作已经做好了"
      ],
      "metadata": {
        "id": "rzAlbYmAU0R7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "transformer模型"
      ],
      "metadata": {
        "id": "lF6wMb1DU4hU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten, Concatenate, Input, Lambda\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization, Embedding\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "physical_devices = tf.config.list_physical_devices('GPU')\n",
        "if len(physical_devices) > 0:\n",
        "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
        "    print(f\"Training on GPU: {physical_devices[0].name}\")\n",
        "else:\n",
        "    print(\"No GPU found, using CPU for training\")\n",
        "\n",
        "# 加载数据\n",
        "input_data = pd.read_csv('/content/drive/MyDrive/UTCI_prediction/data/feature_output.csv')\n",
        "utci_data = pd.read_csv('/content/drive/MyDrive/UTCI_prediction/data/utci_feature.csv')\n",
        "\n",
        "# 合并数据\n",
        "merged_data = pd.merge(input_data, utci_data, on=['region_id','timestamp'])\n",
        "merged_data['timestamp'] = pd.to_datetime(merged_data['timestamp'], format='%Y-%m-%d-%H')\n",
        "data_size = merged_data.shape\n",
        "print(f\"DataFrame 行数: {data_size[0]}, 列数: {data_size[1]}\")\n",
        "\n",
        "# 数据标准化（包括GHI, DNI, DHI）\n",
        "scaler = StandardScaler()\n",
        "numeric_features = merged_data.drop(columns=['UTCI', 'timestamp', 'region_id'])\n",
        "X_scaled = scaler.fit_transform(numeric_features)\n",
        "y = merged_data['UTCI'].values\n",
        "\n",
        "# 确保 region_id 和 y 的数据类型\n",
        "region_ids = merged_data['region_id'].astype(np.int32).values\n",
        "y = y.astype(np.float32)\n",
        "\n",
        "# 按8:1:1划分数据集\n",
        "X_train_numeric, X_temp_numeric, X_train_region, X_temp_region, y_train, y_temp = train_test_split(\n",
        "    X_scaled, region_ids, y, test_size=0.2, random_state=42)\n",
        "\n",
        "X_val_numeric, X_test_numeric, X_val_region, X_test_region, y_val, y_test = train_test_split(\n",
        "    X_temp_numeric, X_temp_region, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# 确定唯一的 region_id 数量\n",
        "num_regions = merged_data['region_id'].nunique()\n",
        "embedding_dim = 64  # 嵌入向量的维度\n",
        "\n",
        "class TransformerBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, mlp_dim, dropout=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
        "        self.mlp = tf.keras.Sequential([\n",
        "            Dense(mlp_dim, activation='relu'),\n",
        "            Dense(d_model),\n",
        "        ])\n",
        "        self.dropout1 = Dropout(dropout)\n",
        "        self.norm1 = LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout2 = Dropout(dropout)\n",
        "        self.norm2 = LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.norm1(inputs + attn_output)\n",
        "        mlp_output = self.mlp(out1)\n",
        "        mlp_output = self.dropout2(mlp_output, training=training)\n",
        "        return self.norm2(out1 + mlp_output)\n",
        "\n",
        "def build_model(input_shape, num_regions, embedding_dim):\n",
        "    inputs_numeric = Input(shape=(input_shape[1],), name='numeric_inputs')\n",
        "    inputs_region = Input(shape=(), name='region_input', dtype=tf.int32)\n",
        "\n",
        "    region_embedding = Embedding(input_dim=num_regions + 1, output_dim=embedding_dim)(inputs_region)\n",
        "    region_embedding = Flatten()(region_embedding)\n",
        "\n",
        "    combined_inputs = Concatenate()([inputs_numeric, region_embedding])\n",
        "\n",
        "    combined_inputs = Lambda(lambda x: tf.expand_dims(x, axis=1))(combined_inputs)\n",
        "\n",
        "    transformer_output = TransformerBlock(d_model=combined_inputs.shape[-1], num_heads=4, mlp_dim=128)(combined_inputs)\n",
        "    flattened_output = Flatten()(transformer_output)\n",
        "    dense_output = Dense(64, activation='relu')(flattened_output)\n",
        "    output = Dense(1)(dense_output)\n",
        "\n",
        "    model = Model(inputs=[inputs_numeric, inputs_region], outputs=output)\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
        "    return model\n",
        "\n",
        "input_shape = X_train_numeric.shape\n",
        "\n",
        "model = build_model(input_shape=input_shape, num_regions=num_regions, embedding_dim=embedding_dim)\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "class PredictionHistory(tf.keras.callbacks.Callback):\n",
        "    def on_train_begin(self, logs=None):\n",
        "        self.epoch_train_pred = []\n",
        "        self.epoch_val_pred = []\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        train_pred = self.model.predict({'numeric_inputs': X_train_numeric, 'region_input': X_train_region}, verbose=0)\n",
        "        val_pred = self.model.predict({'numeric_inputs': X_val_numeric, 'region_input': X_val_region}, verbose=0)\n",
        "        self.epoch_train_pred.append(train_pred)\n",
        "        self.epoch_val_pred.append(val_pred)\n",
        "\n",
        "prediction_history = PredictionHistory()\n",
        "\n",
        "history = model.fit(\n",
        "    {'numeric_inputs': X_train_numeric, 'region_input': X_train_region},\n",
        "    y_train,\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    validation_data=({'numeric_inputs': X_val_numeric, 'region_input': X_val_region}, y_val),\n",
        "    callbacks=[early_stopping, prediction_history]\n",
        ")\n",
        "\n",
        "# 保存模型权重\n",
        "model.save_weights('/content/drive/MyDrive/Transformer_weights.weights.h5')\n",
        "\n",
        "# 评估模型性能\n",
        "y_pred = model.predict({'numeric_inputs': X_test_numeric, 'region_input': X_test_region})\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "# 保存评估数据到 CSV\n",
        "history_df = pd.DataFrame(history.history)\n",
        "history_df['mse'] = mse\n",
        "history_df['r2'] = r2\n",
        "history_df.to_csv('/content/drive/MyDrive/transformer_evaluation_data.csv', index=False)\n",
        "\n",
        "# 打印和绘制结果\n",
        "print(f'MSE: {mse}, R^2: {r2}')\n",
        "\n",
        "# 绘制训练和验证损失曲线\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# 绘制训练和验证MAE曲线\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['mae'], label='Train MAE')\n",
        "plt.plot(history.history['val_mae'], label='Validation MAE')\n",
        "plt.title('Mean Absolute Error')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('MAE')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 绘制预测值与真实值随epoch的变化情况\n",
        "def plot_predictions(epoch_predictions, y_true, title):\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    for epoch, preds in enumerate(epoch_predictions):\n",
        "        plt.plot(y_true, preds, 'o', label=f'Epoch {epoch+1}')\n",
        "    plt.plot(y_true, y_true, 'r--', label='True values')\n",
        "    plt.title(title)\n",
        "    plt.xlabel('True values')\n",
        "    plt.ylabel('Predicted values')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "plot_predictions(prediction_history.epoch_train_pred, y_train, 'Training Predictions vs True Values')\n",
        "plot_predictions(prediction_history.epoch_val_pred, y_val, 'Validation Predictions vs True Values')"
      ],
      "metadata": {
        "id": "5Et2SFwVVDII"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "结果对比"
      ],
      "metadata": {
        "id": "lBGE4t2abf9O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 绘制验证集预测结果与真实结果的对比\n",
        "def plot_comparison(y_true, y_pred, title='Validation Set: Predictions vs True Values'):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(y_true, label='True Values', marker='o', linestyle='-', color='blue')\n",
        "    plt.plot(y_pred, label='Predicted Values', marker='x', linestyle='--', color='red')\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Sample Index')\n",
        "    plt.ylabel('UTCI Value')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# 获取验证集的预测结果\n",
        "y_val_pred = model.predict({'numeric_inputs': X_val_numeric, 'region_input': X_val_region})\n",
        "\n",
        "# 绘制对比图\n",
        "plot_comparison(y_val, y_val_pred.flatten(), title='Validation Set: Predictions vs True Values')"
      ],
      "metadata": {
        "id": "oc7Ubqkcbekp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LSTM算法"
      ],
      "metadata": {
        "id": "pRS2rJpicNII"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Dropout, Embedding, Flatten, Concatenate, Input, LSTM, Lambda\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 加载数据\n",
        "input_data = pd.read_csv('/content/drive/MyDrive/UTCI_prediction/data/feature_output.csv')\n",
        "utci_data = pd.read_csv('/content/drive/MyDrive/UTCI_prediction/data/utci_feature.csv')\n",
        "\n",
        "# 合并数据\n",
        "merged_data = pd.merge(input_data, utci_data, on=['region_id', 'timestamp'])\n",
        "merged_data['timestamp'] = pd.to_datetime(merged_data['timestamp'], format='%Y-%m-%d-%H')\n",
        "\n",
        "# 数据标准化\n",
        "scaler = StandardScaler()\n",
        "numeric_features = merged_data.drop(columns=['UTCI', 'timestamp', 'region_id'])\n",
        "X_scaled = scaler.fit_transform(numeric_features)\n",
        "y = merged_data['UTCI'].values\n",
        "\n",
        "# 确保 region_id 和 y 的数据类型\n",
        "region_ids = merged_data['region_id'].astype(np.int32).values  # 确保为 int32 类型\n",
        "y = y.astype(np.float32)  # 确保 y 为 float32 类型\n",
        "\n",
        "# 按8:1:1划分数据集\n",
        "X_train_numeric, X_temp_numeric, X_train_region, X_temp_region, y_train, y_temp = train_test_split(\n",
        "    X_scaled, region_ids, y, test_size=0.2, random_state=42)\n",
        "\n",
        "X_val_numeric, X_test_numeric, X_val_region, X_test_region, y_val, y_test = train_test_split(\n",
        "    X_temp_numeric, X_temp_region, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# 确定唯一的 region_id 数量\n",
        "num_regions = merged_data['region_id'].nunique()\n",
        "embedding_dim = 64  # 嵌入向量的维度\n",
        "\n",
        "# 构建模型\n",
        "def build_model(input_shape, num_regions, embedding_dim):\n",
        "    inputs_numeric = Input(shape=(input_shape[1],), name='numeric_inputs')\n",
        "    inputs_region = Input(shape=(), name='region_input', dtype=tf.int32)\n",
        "\n",
        "    region_embedding = Embedding(input_dim=num_regions + 1, output_dim=embedding_dim)(inputs_region)\n",
        "    region_embedding = Flatten()(region_embedding)\n",
        "\n",
        "    combined_inputs = Concatenate()([inputs_numeric, region_embedding])\n",
        "\n",
        "    # 使用 Lambda 层包装 tf.expand_dims\n",
        "    combined_inputs = Lambda(lambda x: tf.expand_dims(x, axis=1))(combined_inputs)\n",
        "\n",
        "    lstm_output = LSTM(64)(combined_inputs)  # 替换为LSTM层\n",
        "    dense_output = Dense(64, activation='relu')(lstm_output)\n",
        "    output = Dense(1)(dense_output)\n",
        "\n",
        "    model = Model(inputs=[inputs_numeric, inputs_region], outputs=output)\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
        "    return model\n",
        "\n",
        "# 获取输入形状\n",
        "input_shape = X_train_numeric.shape\n",
        "\n",
        "# 构建和训练模型\n",
        "model = build_model(input_shape=input_shape, num_regions=num_regions, embedding_dim=embedding_dim)\n",
        "\n",
        "# 设置 EarlyStopping 回调\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# 记录每个 epoch 的预测值和真实值\n",
        "class PredictionHistory(tf.keras.callbacks.Callback):\n",
        "    def on_train_begin(self, logs=None):\n",
        "        self.epoch_train_pred = []\n",
        "        self.epoch_val_pred = []\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        train_pred = self.model.predict({'numeric_inputs': X_train_numeric, 'region_input': X_train_region}, verbose=0)\n",
        "        val_pred = self.model.predict({'numeric_inputs': X_val_numeric, 'region_input': X_val_region}, verbose=0)\n",
        "        self.epoch_train_pred.append(train_pred)\n",
        "        self.epoch_val_pred.append(val_pred)\n",
        "\n",
        "prediction_history = PredictionHistory()\n",
        "\n",
        "history = model.fit(\n",
        "    {'numeric_inputs': X_train_numeric, 'region_input': X_train_region},\n",
        "    y_train,\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    validation_data=({'numeric_inputs': X_val_numeric, 'region_input': X_val_region}, y_val),\n",
        "    callbacks=[early_stopping, prediction_history]\n",
        ")\n",
        "\n",
        "# 保存模型权重\n",
        "model.save_weights('/content/drive/MyDrive/LSTM_weights.weights.h5')\n",
        "\n",
        "# 评估模型性能\n",
        "y_pred = model.predict({'numeric_inputs': X_test_numeric, 'region_input': X_test_region})\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "# 保存评估数据到 CSV\n",
        "history_df = pd.DataFrame(history.history)\n",
        "history_df['mse'] = mse\n",
        "history_df['r2'] = r2\n",
        "history_df.to_csv('/content/drive/MyDrive/LSTM_evaluation_data_LSTM.csv', index=False)\n",
        "\n",
        "# 打印和绘制结果\n",
        "print(f'MSE: {mse}, R^2: {r2}')\n",
        "\n",
        "# 绘制训练和验证损失曲线\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# 绘制训练和验证MAE曲线\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['mae'], label='Train MAE')\n",
        "plt.plot(history.history['val_mae'], label='Validation MAE')\n",
        "plt.title('Mean Absolute Error')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('MAE')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 绘制验证集预测结果与真实结果的对比\n",
        "def plot_comparison(y_true, y_pred, title='Validation Set: Predictions vs True Values'):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(y_true, label='True Values', marker='o', linestyle='-', color='blue')\n",
        "    plt.plot(y_pred, label='Predicted Values', marker='x', linestyle='--', color='red')\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Sample Index')\n",
        "    plt.ylabel('UTCI Value')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# 获取验证集的预测结果\n",
        "y_val_pred = model.predict({'numeric_inputs': X_val_numeric, 'region_input': X_val_region})\n",
        "\n",
        "# 绘制对比图\n",
        "plot_comparison(y_val, y_val_pred.flatten(), title='Validation Set: Predictions vs True Values')"
      ],
      "metadata": {
        "id": "3a1Huz5Mb077"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1D卷积神经网络（1D CNN）"
      ],
      "metadata": {
        "id": "obAN1wKIdU8Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Dropout, Embedding, Flatten, Concatenate, Input, Conv1D, GlobalMaxPooling1D, Lambda\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 加载数据\n",
        "input_data = pd.read_csv('/content/drive/MyDrive/UTCI_prediction/data/feature_output.csv')\n",
        "utci_data = pd.read_csv('/content/drive/MyDrive/UTCI_prediction/data/utci_feature.csv')\n",
        "\n",
        "# 合并数据\n",
        "merged_data = pd.merge(input_data, utci_data, on=['region_id', 'timestamp'])\n",
        "merged_data['timestamp'] = pd.to_datetime(merged_data['timestamp'], format='%Y-%m-%d-%H')\n",
        "\n",
        "# 数据标准化\n",
        "scaler = StandardScaler()\n",
        "numeric_features = merged_data.drop(columns=['UTCI', 'timestamp', 'region_id'])\n",
        "X_scaled = scaler.fit_transform(numeric_features)\n",
        "y = merged_data['UTCI'].values\n",
        "\n",
        "# 确保 region_id 和 y 的数据类型\n",
        "region_ids = merged_data['region_id'].astype(np.int32).values\n",
        "y = y.astype(np.float32)\n",
        "\n",
        "# 按8:1:1划分数据集\n",
        "X_train_numeric, X_temp_numeric, X_train_region, X_temp_region, y_train, y_temp = train_test_split(\n",
        "    X_scaled, region_ids, y, test_size=0.2, random_state=42)\n",
        "\n",
        "X_val_numeric, X_test_numeric, X_val_region, X_test_region, y_val, y_test = train_test_split(\n",
        "    X_temp_numeric, X_temp_region, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# 确定唯一的 region_id 数量\n",
        "num_regions = merged_data['region_id'].nunique()\n",
        "embedding_dim = 64  # 嵌入向量的维度\n",
        "\n",
        "# 构建1D CNN模型\n",
        "def build_model(input_shape, num_regions, embedding_dim):\n",
        "    inputs_numeric = Input(shape=(input_shape[1],), name='numeric_inputs')\n",
        "    inputs_region = Input(shape=(), name='region_input', dtype=tf.int32)\n",
        "\n",
        "    region_embedding = Embedding(input_dim=num_regions + 1, output_dim=embedding_dim)(inputs_region)\n",
        "    region_embedding = Flatten()(region_embedding)\n",
        "\n",
        "    combined_inputs = Concatenate()([inputs_numeric, region_embedding])\n",
        "\n",
        "    # 使用 Lambda 层包装 tf.expand_dims\n",
        "    combined_inputs = Lambda(lambda x: tf.expand_dims(x, axis=2))(combined_inputs)\n",
        "\n",
        "    conv_output = Conv1D(filters=64, kernel_size=3, activation='relu')(combined_inputs)\n",
        "    pooled_output = GlobalMaxPooling1D()(conv_output)\n",
        "    dense_output = Dense(64, activation='relu')(pooled_output)\n",
        "    output = Dense(1)(dense_output)\n",
        "\n",
        "    model = Model(inputs=[inputs_numeric, inputs_region], outputs=output)\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
        "    return model\n",
        "\n",
        "# 获取输入形状\n",
        "input_shape = X_train_numeric.shape\n",
        "\n",
        "# 构建和训练模型\n",
        "model = build_model(input_shape=input_shape, num_regions=num_regions, embedding_dim=embedding_dim)\n",
        "\n",
        "# 设置 EarlyStopping 回调\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "history = model.fit(\n",
        "    {'numeric_inputs': X_train_numeric, 'region_input': X_train_region},\n",
        "    y_train,\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    validation_data=({'numeric_inputs': X_val_numeric, 'region_input': X_val_region}, y_val),\n",
        "    callbacks=[early_stopping]\n",
        ")\n",
        "\n",
        "# 评估模型性能\n",
        "y_pred = model.predict({'numeric_inputs': X_test_numeric, 'region_input': X_test_region})\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "# 保存模型权重\n",
        "model.save_weights('/content/drive/MyDrive/CNN_weights.h5')\n",
        "\n",
        "# 保存评估数据到 CSV\n",
        "history_df = pd.DataFrame(history.history)\n",
        "history_df['mse'] = mse\n",
        "history_df['r2'] = r2\n",
        "history_df.to_csv('/content/drive/MyDrive/CNN_evaluation_data.csv', index=False)\n",
        "\n",
        "# 打印和绘制结果\n",
        "print(f'MSE: {mse}, R^2: {r2}')\n",
        "\n",
        "# 绘制训练和验证损失曲线\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['mae'], label='Train MAE')\n",
        "plt.plot(history.history['val_mae'], label='Validation MAE')\n",
        "plt.title('Mean Absolute Error')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('MAE')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9Bjk4PSHdVtq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " GRU (Gated Recurrent Unit)"
      ],
      "metadata": {
        "id": "bX1GnjGldX8M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import GRU\n",
        "\n",
        "# 构建GRU模型\n",
        "def build_model(input_shape, num_regions, embedding_dim):\n",
        "    inputs_numeric = Input(shape=(input_shape[1],), name='numeric_inputs')\n",
        "    inputs_region = Input(shape=(), name='region_input', dtype=tf.int32)\n",
        "\n",
        "    region_embedding = Embedding(input_dim=num_regions + 1, output_dim=embedding_dim)(inputs_region)\n",
        "    region_embedding = Flatten()(region_embedding)\n",
        "\n",
        "    combined_inputs = Concatenate()([inputs_numeric, region_embedding])\n",
        "\n",
        "    # 使用 Lambda 层包装 tf.expand_dims\n",
        "    combined_inputs = Lambda(lambda x: tf.expand_dims(x, axis=1))(combined_inputs)\n",
        "\n",
        "    gru_output = GRU(64)(combined_inputs)\n",
        "    dense_output = Dense(64, activation='relu')(gru_output)\n",
        "    output = Dense(1)(dense_output)\n",
        "\n",
        "    model = Model(inputs=[inputs_numeric, inputs_region], outputs=output)\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
        "    return model\n",
        "\n",
        "# 获取输入形状\n",
        "input_shape = X_train_numeric.shape\n",
        "\n",
        "# 构建和训练模型\n",
        "model = build_model(input_shape=input_shape, num_regions=num_regions, embedding_dim=embedding_dim)\n",
        "\n",
        "# 设置 EarlyStopping 回调\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "history = model.fit(\n",
        "    {'numeric_inputs': X_train_numeric, 'region_input': X_train_region},\n",
        "    y_train,\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    validation_data=({'numeric_inputs': X_val_numeric, 'region_input': X_val_region}, y_val),\n",
        "    callbacks=[early_stopping]\n",
        ")\n",
        "\n",
        "# 后续步骤与之前的示例相同"
      ],
      "metadata": {
        "id": "Of9us3s_dbKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "混合模型：CNN + RNN"
      ],
      "metadata": {
        "id": "SPtDPPlcddq8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import GRU, Conv1D, GlobalMaxPooling1D\n",
        "\n",
        "# 构建CNN + GRU混合模型\n",
        "def build_model(input_shape, num_regions, embedding_dim):\n",
        "    inputs_numeric = Input(shape=(input_shape[1],), name='numeric_inputs')\n",
        "    inputs_region = Input(shape=(), name='region_input', dtype=tf.int32)\n",
        "\n",
        "    region_embedding = Embedding(input_dim=num_regions + 1, output_dim=embedding_dim)(inputs_region)\n",
        "    region_embedding = Flatten()(region_embedding)\n",
        "\n",
        "    combined_inputs = Concatenate()([inputs_numeric, region_embedding])\n",
        "\n",
        "    combined_inputs = Lambda(lambda x: tf.expand_dims(x, axis=2))(combined_inputs)\n",
        "\n",
        "    conv_output = Conv1D(filters=64, kernel_size=3, activation='relu')(combined_inputs)\n",
        "    pooled_output = GlobalMaxPooling1D()(conv_output)\n",
        "\n",
        "    lstm_output = GRU(64)(Lambda(lambda x: tf.expand_dims(x, axis=1))(pooled_output))\n",
        "    dense_output = Dense(64, activation='relu')(lstm_output)\n",
        "    output = Dense(1)(dense_output)\n",
        "\n",
        "    model = Model(inputs=[inputs_numeric, inputs_region], outputs=output)\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
        "    return model\n",
        "\n",
        "# 获取输入形状\n",
        "input_shape = X_train_numeric.shape\n",
        "\n",
        "# 构建和训练模型\n",
        "model = build_model(input_shape=input_shape, num_regions=num_regions, embedding_dim=embedding_dim)\n",
        "\n",
        "# 设置 EarlyStopping 回调\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "history = model.fit(\n",
        "    {'numeric_inputs': X_train_numeric, 'region_input': X_train_region},\n",
        "    y_train,\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    validation_data=({'numeric_inputs': X_val_numeric, 'region_input': X_val_region}, y_val),\n",
        "    callbacks=[early_stopping]\n",
        ")\n",
        "\n",
        "# 后续步骤与之前的示例相同"
      ],
      "metadata": {
        "id": "trZS923Udf2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MLP (Multi-Layer Perceptron)"
      ],
      "metadata": {
        "id": "EHdyS0hodjo9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 构建MLP模型\n",
        "def build_model(input_shape, num_regions, embedding_dim):\n",
        "    inputs_numeric = Input(shape=(input_shape[1],), name='numeric_inputs')\n",
        "    inputs_region = Input(shape=(), name='region_input', dtype=tf.int32)\n",
        "\n",
        "    region_embedding = Embedding(input_dim=num_regions + 1, output_dim=embedding_dim)(inputs_region)\n",
        "    region_embedding = Flatten()(region_embedding)\n",
        "\n",
        "    combined_inputs = Concatenate()([inputs_numeric, region_embedding])\n",
        "\n",
        "    dense_output = Dense(128, activation='relu')(combined_inputs)\n",
        "    dense_output = Dropout(0.2)(dense_output)\n",
        "    dense_output = Dense(64, activation='relu')(dense_output)\n",
        "    output = Dense(1)(dense_output)\n",
        "\n",
        "    model = Model(inputs=[inputs_numeric, inputs_region], outputs=output)\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
        "    return model\n",
        "\n",
        "# 获取输入形状\n",
        "input_shape = X_train_numeric.shape\n",
        "\n",
        "# 构建和训练模型\n",
        "model = build_model(input_shape=input_shape, num_regions=num_regions, embedding_dim=embedding_dim)\n",
        "\n",
        "# 设置 EarlyStopping 回调\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "history = model.fit(\n",
        "    {'numeric_inputs': X_train_numeric, 'region_input': X_train_region},\n",
        "    y_train,\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    validation_data=({'numeric_inputs': X_val_numeric, 'region_input': X_val_region}, y_val),\n",
        "    callbacks=[early_stopping]\n",
        ")\n",
        "\n",
        "# 后续步骤与之前的示例相同"
      ],
      "metadata": {
        "id": "MSPP7aZndkIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "以下是对比1D CNN、GRU、CNN+GRU、MLP几种模型在时序数据处理中的优缺点和适用场景的表格：\n",
        "\n",
        "| **模型**        | **架构类型** | **优点** | **缺点** | **适用场景** |\n",
        "|-----------------|--------------|----------|----------|--------------|\n",
        "| **1D CNN**      | 卷积神经网络 (CNN) | - 擅长提取局部特征<br>- 计算效率高，适合并行处理<br>- 能捕获短期依赖关系 | - 不擅长处理长时间依赖<br>- 可能无法充分利用时序信息 | - 时序信号处理<br>- 短时间依赖特征提取 |\n",
        "| **GRU**         | 循环神经网络 (RNN) | - 较轻量的RNN结构<br>- 擅长捕获长时间依赖<br>- 比LSTM更快，参数更少 | - 计算复杂度较高，难以并行<br>- 在非常长序列上仍有局限性 | - 长时间依赖任务<br>- 时间序列预测<br>- 自然语言处理 |\n",
        "| **CNN + GRU**   | 混合模型 (CNN + RNN) | - 结合了CNN和RNN的优点<br>- CNN处理局部特征，RNN处理全局时间依赖<br>- 通常表现优于单一模型 | - 复杂度较高<br>- 需要更多计算资源<br>- 训练时间长 | - 需要同时提取局部和全局特征的任务<br>- 复杂的时序数据处理 |\n",
        "| **MLP**         | 多层感知器 (全连接神经网络) | - 结构简单，易于实现<br>- 适合小型数据集<br>- 训练速度快 | - 不适合捕捉时间依赖关系<br>- 无法充分利用时序数据的特性 | - 非时间依赖的任务<br>- 简单的回归或分类任务 |\n",
        "\n",
        "### 详细说明：\n",
        "1. **1D CNN**:\n",
        "   - **优点**：1D CNN 通过卷积操作能够有效地提取序列中的局部模式，特别适用于具有局部依赖关系的时序数据。此外，由于卷积运算的高度并行性，计算效率较高，适合在较大数据集上进行快速训练。\n",
        "   - **缺点**：1D CNN 主要关注局部特征，而不能很好地捕捉长时间的依赖关系。如果你的数据具有较长的时间依赖性，CNN 可能会丢失一些全局信息。\n",
        "   - **适用场景**：适合处理短期依赖的时序信号，如语音处理或传感器数据分析。\n",
        "\n",
        "2. **GRU**:\n",
        "   - **优点**：GRU 是 RNN 的一种变体，能够有效地捕捉长时间依赖关系，但相比 LSTM 参数更少，计算速度更快。GRU 在处理长序列时表现良好，并且由于结构较简单，适合快速原型开发。\n",
        "   - **缺点**：尽管 GRU 可以处理长时间依赖，但与 CNN 相比，计算复杂度较高，并且由于其序列化处理的特性，无法很好地并行化，训练时间可能较长。\n",
        "   - **适用场景**：适用于时间依赖性强的任务，如时间序列预测、文本生成等。\n",
        "\n",
        "3. **CNN + GRU**:\n",
        "   - **优点**：这种混合模型结合了 CNN 和 GRU 的优点，CNN 负责提取序列的局部特征，GRU 处理全局的时间依赖关系。通常情况下，这种组合模型在复杂的时序数据处理任务中表现更优。\n",
        "   - **缺点**：由于组合了两种模型，这种方法的复杂度较高，需要更多的计算资源和更长的训练时间。\n",
        "   - **适用场景**：适用于需要同时处理局部特征和全局依赖的复杂时序任务，如视频分析或多模态数据分析。\n",
        "\n",
        "4. **MLP**:\n",
        "   - **优点**：MLP 是一种结构简单的全连接神经网络，适合处理小型数据集，训练速度快，适合快速测试模型。\n",
        "   - **缺点**：由于 MLP 无法处理序列数据的时间依赖性，因此在时序数据任务中表现较差，无法充分利用时间序列中的相关信息。\n",
        "   - **适用场景**：适用于非时序性任务，或简单的回归和分类问题，如预测单一变量或分类静态数据。\n",
        "\n",
        "### 总结：\n",
        "- **如果数据具有显著的时间依赖性**，如时间序列预测任务，**GRU** 和 **CNN+GRU** 是较好的选择。GRU 适合长时间依赖性的数据，而 CNN+GRU 则能同时捕获局部和全局特征。\n",
        "- **如果你只关心短时间依赖关系或数据的局部模式**，**1D CNN** 是一个高效的选择。\n",
        "- **如果数据是静态的或时间依赖性不强**，**MLP** 可以提供一个简单有效的解决方案。\n",
        "\n",
        "根据你的具体应用场景，可以选择其中一种或结合多种方法进行实验，找到最适合的模型。"
      ],
      "metadata": {
        "id": "wrRLyXQleMDG"
      }
    }
  ]
}